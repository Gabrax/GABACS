% w nawiasie kwadratowym wpisujemy rodzaj pracy: 
% magisterska, licencjacka, inzynierska
\documentclass[magisterska]{pracadypl}

%% ważne definicje %%
\usepackage{tgtermes}
\usepackage[T1]{fontenc}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\input glyphtounicode
\pdfgentounicode=1
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{graphicx}
\setcounter{secnumdepth}{4}  % Ensure subsubsection is numbered
\setcounter{tocdepth}{4}     % Ensure subsubsection appears in TOC
\usepackage{titlesec}
\usepackage{color}
\usepackage{xcolor}
\usepackage{float}
\usepackage{listings}
\usepackage{parskip}
\usepackage{xcolor} % optional, for color
\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\bibliographystyle{plain}

\def\mgr{magisterska}
\def\lic{licencjacka}
\def\inz{inżynierska}

\def\sk{Słowa kluczowe}
\def\kw{Keywords}
\def\et{Title in English}
%% koniec ważnych definicji %%

%% wypełnia Autor pracy %%

%autor pracy
\author{Gabriel Ozeg}
%numer albumu
\nralbumu{395263}
%tytuł pracy
\title{System antykolizyjny na mikroprocesorze Raspberry Pi}
%kierunek studiów
\kierunek{Informatyka}
%promotor w dopełniaczu
\opiekun{dr Paweł Zajączkowski}
\katedra{Katedra Informatyki Stosowanej}
%rok
\date{2025}
%Słowa kluczowe:
\slkluczowe{Przetwarzanie obrazu, Głębia obrazu, Metody pomiaru odległości w czasie rzeczywistym, Zastosowania w robotyce}
%tytuł po angielsku
\tytulang{Collision avoidance system on Raspberry Pi microprocessor}
%słowa kluczowe po angielsku
\keywords{Image Processing, Image Depth, Real-Time Distance Measurement Methods, Applications in Robotics}
%% koniec ważnych definicji %%

%% APD %%
%% w systemie APD należy jeszcze wpisać, poza powyższymi informacjami, streszczenie oraz streszczenie w języku angielskim  %%


%%% definicje %%%
\def\pd{\noindent \textbf{Dowód.~}} %%początek dowodu
\def\kd{\hfill\mbox{$\rule{2mm}{2mm}$}} %%koniec dowodu
\newtheorem{defi}{Definicja}[section]
\newtheorem{uwaga}{Uwaga}[section]
\newtheorem{tw}{Twierdzenie}[section]
\newtheorem{lem}{Lemat}[section]
\newtheorem{wn}{Wniosek}[section]
\renewcommand\thetw{\thesection.\arabic{tw}.}
\renewcommand\thedefi{\thesection.\arabic{defi}.}
\renewcommand\theuwaga{\thesection.\arabic{uwaga}.}
\renewcommand\thetw{\thesection.\arabic{tw}.}
\renewcommand\thelem{\thesection.\arabic{lem}.}
\renewcommand\thewn{\thesection.\arabic{wn}.}
%
\definecolor{wmiigreen}{rgb}{0.0, 0.5, 0.0}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{wmiigreen}}{\chaptertitlename\ \thechapter}{10pt}{\Huge}
 %
\linespread{1.3}
%%% koniec definicji %%%

\begin{document}

\lstdefinestyle{mypython}{
    language=Python,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{red}\bfseries,
    stringstyle=\color{green},
    commentstyle=\color{blue},
    inputencoding=utf8,
    literate=
      {ą}{{\k{a}}}1 {Ą}{{\k{A}}}1
      {ć}{{\'c}}1 {Ć}{{\'C}}1
      {ę}{{\k{e}}}1 {Ę}{{\k{E}}}1
      {ł}{{\l{}}}1 {Ł}{{\L{}}}1
      {ń}{{\'n}}1 {Ń}{{\'N}}1
      {ó}{{\'o}}1 {Ó}{{\'O}}1
      {ś}{{\'s}}1 {Ś}{{\'S}}1
      {ź}{{\'z}}1 {Ź}{{\'Z}}1
      {ż}{{\.z}}1 {Ż}{{\.Z}}1
}

\maketitle
\tableofcontents
\newpage

\chapter{Wstęp}

Przetwarzanie obrazu cyfrowego stanowi jedną z kluczowych dziedzin współczesnej informatyki oraz inżynierii komputerowej, znajdującą zastosowanie w wielu obszarach życia codziennego, przemysłu i nauki. Głównym celem tej dziedziny jest analiza, przekształcanie oraz interpretacja danych wizualnych przy użyciu metod matematycznych, algorytmów komputerowych oraz technik sztucznej inteligencji. Przetwarzanie obrazu umożliwia nie tylko poprawę jakości obrazów, ale także automatyczną ekstrakcję informacji, segmentację obiektów, rozpoznawanie kształtów czy estymację głębi sceny.

Systemy wizyjne znajdują zastosowanie m.in. w diagnostyce medycznej (np. analiza obrazów RTG i MRI), przemyśle (automatyczna kontrola jakości), systemach bezpieczeństwa (rozpoznawanie twarzy i analiza zachowań) oraz w autonomicznych pojazdach i robotyce, gdzie odpowiadają za detekcję przeszkód i wspomaganie decyzji nawigacyjnych w czasie rzeczywistym. Szczególne znaczenie zyskują implementacje systemów wizyjnych na platformach o ograniczonych zasobach sprzętowych, takich jak mikrokontrolery czy komputery jednopłytkowe.

Celem niniejszego projektu jest implementacja oraz ocena działania \textbf{systemu antykolizyjnego} opartego na przetwarzaniu obrazu stereoskopowego w czasie rzeczywistym. Główną platformą obliczeniową jest \textbf{Raspberry Pi 5} – komputer jednopłytkowy nowej generacji, oferujący około 50\% wyższą wydajność w porównaniu do swojego poprzednika, co czyni go obiecującą jednostką dla zastosowań typu \textit{edge computing}.

System bazuje na wykorzystaniu \textbf{stereowizji}, czyli przetwarzania obrazu z dwóch zsynchronizowanych kamer w celu uzyskania mapy dysparycji i wyznaczenia odległości do obiektów w scenie. W przypadku wykrycia przeszkody znajdującej się zbyt blisko, system podejmuje decyzję o zatrzymaniu pojazdu poprzez fizyczne odłączenie zasilania jego silnika napędowego, co ma na celu uniknięcie kolizji.

Projekt został zrealizowany przy użyciu następujących komponentów sprzętowych:

\begin{itemize}
  \item \textbf{Raspberry Pi 5} – odpowiada za przetwarzanie obrazu stereoskopowego, analizę głębi oraz sterowanie logiką decyzyjną systemu.
    \begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
      \centering  % Centers the image
      \includegraphics[width=0.5\textwidth]{images/rpi5.png}  % Adjust the path and width as necessary
      \captionsetup{font=footnotesize}
      \caption[Płytka Raspberry Pi 5. https://www.hackatronic.com/wp-content/uploads/2024/03/Raspberry-Pi-5-Pinout--1210x642.jpg]{Płytka Raspberry Pi 5.}
    \end{figure}

  \item \textbf{Kamera stereo} – dostarcza zsynchronizowane obrazy z dwóch perspektyw, umożliwiające wyznaczenie mapy głębi.
    \begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
      \centering  % Centers the image
      \includegraphics[width=0.5\textwidth]{images/MAINSTEREO.png}  % Adjust the path and width as necessary
      \captionsetup{font=footnotesize}
      \caption[Kamera stereowizyjna. https://cell-kom.com/inne/21454-kamera-internetowa-full-hd-b16-1080p-5900217390350.html]{Kamera stereowizyjna}
      \label{fig:mono}  % Optional: use to refer to this image later in the text
    \end{figure}

  \item \textbf{Moduł UPS HAT (B) firmy Waveshare} – zapewnia nieprzerwane zasilanie, umożliwiając pracę systemu w warunkach mobilnych oraz zwiększając jego niezawodność.
    \begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
      \centering  % Centers the image
      \includegraphics[width=0.5\textwidth]{images/upsfront.png}  % Adjust the path and width as necessary
      \captionsetup{font=footnotesize}
      \caption[Moduł zasilający. https://www.waveshare.com/wiki/UPS-HAT-(B)]{Moduł zasilający.}
    \end{figure}

  \item \textbf{Moduł przekaźnikowy} – odpowiada za fizyczne odłączenie zasilania jednostki napędowej w przypadku wykrycia zagrożenia kolizją.

    \begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
      \centering  % Centers the image
      \includegraphics[width=0.4\textwidth]{images/relay.png}  % Adjust the path and width as necessary
      \captionsetup{font=footnotesize}
      \caption[Moduł przekaźnika. https://l1nq.com/hQOm9]{Moduł przekaźnika.}
    \end{figure}

  \item \textbf{Zabawkowy samochód elektryczny} – służy jako platforma testowa dla systemu, umożliwiając przeprowadzanie eksperymentów w warunkach laboratoryjnych oraz polowych.
    \begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
      \centering  % Centers the image
      \includegraphics[width=0.4\textwidth]{images/auto.png}  % Adjust the path and width as necessary
      \captionsetup{font=footnotesize}
      \caption[Pojazd projektu. Opracowanie własne]{Pojazd projektu.}
    \end{figure}

\end{itemize}

W ramach realizacji projektu przeprowadzona została analiza skuteczności oraz wydajności systemu w warunkach rzeczywistych. Zakres badań obejmował:

\begin{itemize}
  \item ocenę dokładności estymacji głębi oraz detekcji przeszkód,
  \item pomiar obciążenia obliczeniowego Raspberry Pi 5 i ocena jego zdolności do pracy w czasie rzeczywistym,
  \item analizę responsywności systemu w kontekście szybkości reakcji na przeszkody,
  \item testy stabilności zasilania przy wykorzystaniu UPS HAT w środowisku mobilnym,
  \item ocenę możliwości zastosowania projektu w celach komercyjnych, m.in. w robotyce mobilnej, pojazdach magazynowych czy inteligentnych systemach bezpieczeństwa.
\end{itemize}

Większa wartość paralaksy oznacza mniejszą odległość obiektu od kamery.

Opracowany system został zaimplementowany w języku \texttt{Python} z wykorzystaniem biblioteki \texttt{OpenCV} do przetwarzania obrazów oraz biblioteki \texttt{rpi.lgpio} do obsługi pinów wejścia/wyjścia \textit{Raspberry Pi}.

Typowy proces obliczania głębi metodą stereowizyjną obejmuje następujące kluczowe kroki:

\begin{enumerate}
    \item \textbf{Rektyfikacja obrazów} – transformacja obrazów w taki sposób, aby linie epipolarne były równoległe i współpłaszczyznowe względem osi \(Y\), co umożliwia wyszukiwanie korespondencji wyłącznie wzdłuż osi \(X\).
    \item \textbf{Wyszukiwanie korespondencji} – identyfikacja odpowiadających sobie punktów na obrazach lewym i prawym, prowadząca do wygenerowania \textit{mapy dysparycji} przedstawiającej różnice położenia na osi \(X\).
    \item \textbf{Filtr WLS} – filtr wygładzający, który redukuje szumy w mapie dysparycji, zachowując jednocześnie ostrość krawędzi obiektów.
    \item \textbf{Triangulacja} – przekształcenie mapy dysparycji na mapę głębi przy wykorzystaniu parametrów geometrycznych układu kamer.
\end{enumerate}

System uruchamiany jest automatycznie wraz ze startem systemu operacyjnego \textit{Raspberry Pi}. Moduł przekaźnikowy domyślnie pozostaje w stanie odłączenia, a po aktywacji systemu obie kamery są inicjalizowane w trybie pracy ciągłej.

\chapter{Natura kamery}

Kamery rejestrują promienie świetlne z otoczenia. Zasadniczo kamera działa podobnie jak ludzkie oko — odbite promienie światła trafiają do oka i są skupiane na siatkówce.

Najprostszym modelem kamery jest tzw. „kamera otworkowa” \cite{otworkowa}. To dobre uproszczenie pozwalające zrozumieć podstawy działania kamery. W tym modelu wszystkie promienie świetlne są blokowane przez ścianki, a tylko te przechodzące przez mały otwór trafiają na powierzchnię światłoczułą wewnątrz kamery, tworząc odwrócony obraz. Poniższa ilustracja przedstawia tę zasadę.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/light.png}
    \captionsetup{font=footnotesize}
    \caption[Odwrócenie obrazu przez soczewkę. https://funsizephysics.com/use-light-turn-world-upside/]{Odwrócenie obrazu przez soczewkę}
    \label{fig:rpi}
\end{figure}

Choć ten model jest prosty, nie nadaje się dobrze do rejestrowania wystarczającej ilości światła przy krótkim czasie naświetlania. Dlatego w kamerach stosuje się soczewki, które skupiają promienie świetlne w jednym punkcie. Niestety, powoduje to powstawanie zniekształceń.

Istnieją dwa główne rodzaje zniekształceń:

\begin{itemize}
  \item Zniekształcenie promieniowe(radialne) — spowodowane kształtem soczewki,występujące symetrycznie względem środka obrazu.

  \item Zniekształcenie styczne(tangencjalne) — wynikające z niedoskonałości montażu lub geometrii kamery.
\end{itemize}

Obrazy zniekształcone w ten sposób można skorygować za pomocą metod matematycznych. Proces kalibracji pozwala stworzyć model geometrii kamery oraz model zniekształceń obiektywu. Te modele określają tzw. parametry wewnętrzne kamery.

\section{Ogniskowa obiektywu}

Względny rozmiar obrazu rzutowanego na powierzchnię w kamerze zależy od ogniskowej \cite{ogniskowa}.

W modelu otworkowym ogniskowa to odległość między otworem, przez który przechodzi światło, a obszarem, na który rzutowany jest obraz.

Aby wyznaczyć, jak duży będzie obraz obiektu na płaszczyźnie projekcji, korzystamy z \textbf{twierdzenia Talesa}.  
Dlaczego?

Obiekt znajdujący się w przestrzeni oraz jego rzut w kamerze tworzą dwa \textbf{podobne trójkąty}:
\begin{itemize}
    \item Jeden utworzony przez obiekt o wysokości \( X \), znajdujący się w odległości \( Z \) od kamery.
    \item Drugi utworzony przez obraz tego obiektu na płaszczyźnie znajdującej się w odległości \( f \) od otworu kamery, którego wysokość to \( x \).
\end{itemize}

Ponieważ kąty tych trójkątów są identyczne (wierzchołek kąta w otworze kamery) i odpowiadające sobie boki są proporcjonalne, możemy zastosować twierdzenie Talesa:

\[
\frac{x}{f} = \frac{X}{Z} \Rightarrow x = f \cdot \frac{X}{Z}
\]

Obraz na matrycy jest odwrócony, dlatego często w literaturze pojawia się wersja ze znakiem minus:

\[
-x = f \cdot \frac{X}{Z}
\]

\begin{itemize}
  \item \textbf{$x$}: obraz obiektu (znak minus wynika z tego, że obraz jest odwrócony)
  \item \textbf{$X$}: rozmiar obiektu
  \item \textbf{$Z$}: odległość od otworu do obiektu
  \item \textbf{$f$}: ogniskowa, odległość od otworu do obrazu
\end{itemize}

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.5\textwidth]{images/pinhole.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Model kamery otworkowej. Learning OpenCV 3, O'Reilly, Str. 639]{Model kamery otworkowej}
    \label{fig:rpi}  % Optional: use to refer to this image later in the text
\end{figure}

Ponieważ soczewka nie jest idealnie wyśrodkowana, wprowadzono dwa parametry, $C_x$ i $C_y$, oznaczające odpowiednio poziome i pionowe przemieszczenie soczewki.
Ogniskowa na osiach $X$ i $Y$ są również różna, ponieważ obszar obrazu jest prostokątny. Daje to następujący wzór na położenie obiektu na powierzchni.

\[
x_{\text{screen}} = f_x \left( \frac{X}{Z} \right) + c_x, \quad
y_{\text{screen}} = f_y \left( \frac{Y}{Z} \right) + c_y
\]

Rzutowane punkty świata rzeczywistego na powierzchnię obrazu można zatem modelować w następujący sposób. $M$ jest tutaj macierzą wewnętrzną.

\bigskip

Punkt w przestrzeni 3D:
\[
Q = \begin{bmatrix} X \\ Y \\ Z \end{bmatrix}
\]

Po rzutowaniu za pomocą macierzy kamery $M$:
\[
M = \begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
\]

otrzymujemy punkt obrazu w jednorodnych współrzędnych:
\[
q = M \cdot \begin{bmatrix}
\frac{X}{Z} \\
\frac{Y}{Z} \\
1
\end{bmatrix}
=
\begin{bmatrix}
f_x \cdot \frac{X}{Z} + c_x \\
f_y \cdot \frac{Y}{Z} + c_y \\
1
\end{bmatrix}
\]

Po normalizacji współrzędnych jednorodnych:
\[
x = \frac{q_x}{q_w} = f_x \cdot \frac{X}{Z} + c_x
\quad , \quad
y = \frac{q_y}{q_w} = f_y \cdot \frac{Y}{Z} + c_y
\]

Dla ogólnej macierzy projekcji:
\[
P = M \cdot [R \;|\; t]
\]

Macierz \( P = M[R | t] \) nazywana jest macierzą projekcji kamery i zawiera zarówno informacje o parametrach wewnętrznych kamery (macierz \( M \)), jak i jej położeniu i orientacji w przestrzeni (macierze \( R \) i \( t \)).

Rzut punktu \( Q_h = \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} \) przy pomocy tej macierzy daje nam współrzędne punktu\\ \( q = \begin{bmatrix} x' \\ y' \\ w \end{bmatrix} \) w jednorodnych współrzędnych, które po znormalizowaniu (\( x = \frac{x'}{w} \)) dają końcowe położenie punktu na obrazie.

\[
x = \frac{x'}{w}, \quad y = \frac{y'}{w}
\]

Współrzędne \( x, y \) po normalizacji są współrzędnymi piksela na płaszczyźnie obrazu, czyli miejscem, gdzie dany punkt 3D zostanie odwzorowany na zdjęciu lub klatce z kamery. Uwzględniają one zarówno parametry geometryczne obiektywu (ogniskowe \( f_x, f_y \)) jak i przesunięcia optycznego środka obrazu (\( c_x, c_y \)).

\section{Dystorsja obrazu}

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.5\textwidth]{images/dystorsje.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Rodzaje dystorsji. https://beafoto.pl/dystorsja]{Rodzaje dystorsji}
    \label{fig:rpi}  % Optional: use to refer to this image later in the text
\end{figure}

Teoretycznie możliwe jest zbudowanie obiektywu, który nie powoduje zniekształceń, np. przy użyciu soczewki parabolicznej. W praktyce jednak znacznie łatwiej i taniej jest wytwarzać soczewki sferyczne, które niestety powodują różne typy zniekształceń geometrycznych obrazu \cite{dystorsja}.

Aby móc opisać i skorygować te zniekształcenia, punkt obrazu wyrażony w pikselach \( (u, v) \) przekształca się najpierw do znormalizowanego układu współrzędnych kamery:

\[
x = \frac{u - c_x}{f_x}, \quad y = \frac{v - c_y}{f_y}
\]

Gdzie:
\begin{itemize}
    \item \( (c_x, c_y) \) — współrzędne głównego punktu optycznego (środka obrazu),
    \item \( (f_x, f_y) \) — ogniskowe w poziomie i pionie wyrażone w pikselach,
    \item \( (x, y) \) — znormalizowane współrzędne względem osi optycznej kamery.
\end{itemize}

\subsection*{Zniekształcenia promieniowe}

Zniekształcenia promieniowe (ang. \textit{radial distortion}) \cite{promieniowe} są symetryczne względem środka obrazu i ich wpływ rośnie wraz z odległością od środka. Dla punktów znormalizowanych odległość ta dana jest przez:

\[
r^2 = x^2 + y^2
\]

Efekt zniekształcenia można modelować jako nieliniową funkcję \( D(r) \), która modyfikuje współrzędne punktu zależnie od \( r \). Ponieważ funkcja \( D(r) \) nie jest znana analitycznie, stosujemy jej rozwinięcie Taylora w punkcie \( r = 0 \):

\[
D(r) = 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 + \ldots
\]

Ograniczamy się zwykle do trzeciego rzędu (\( r^6 \)), ponieważ kolejne składniki mają marginalny wpływ, a zwiększają złożoność obliczeń. Po uwzględnieniu tej funkcji korekta zniekształcenia promieniowego ma postać:

\begin{align*}
x_{\text{radial}} &= x \cdot \left(1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \right) \\
y_{\text{radial}} &= y \cdot \left(1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \right)
\end{align*}

\subsection*{Zniekształcenia styczne (tangencjalne)}

Zniekształcenia styczne pojawiają się w wyniku niewspółosiowości soczewek (np. przesunięcia lub nachylenia względem osi optycznej). Ich model opiera się na dwóch parametrach \( p_1 \) i \( p_2 \):

\begin{align*}
x_{\text{tangential}} &= x + \left[2p_1 x y + p_2 (r^2 + 2x^2)\right] \\
y_{\text{tangential}} &= y + \left[p_1 (r^2 + 2y^2) + 2p_2 x y\right]
\end{align*}

\subsection*{Pełna korekta punktu znormalizowanego}

Sumując oba typy zniekształceń, uzyskujemy skorygowane współrzędne punktu w układzie znormalizowanym:

\begin{align*}
x_{\text{corrected}} &= x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + 2p_1 x y + p_2 (r^2 + 2x^2) \\
y_{\text{corrected}} &= y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + p_1 (r^2 + 2y^2) + 2p_2 x y
\end{align*}

\subsection*{Powrót do współrzędnych obrazu}

Na końcu przekształcamy punkt z powrotem do układu pikselowego:

\[
u_{\text{corrected}} = f_x \cdot x_{\text{corrected}} + c_x, \quad
v_{\text{corrected}} = f_y \cdot y_{\text{corrected}} + c_y
\]

W ten sposób otrzymujemy ostateczną, skorygowaną pozycję punktu obrazu, która uwzględnia wpływ obu typów zniekształceń optycznych.

\section*{Dane z obrazu kamery}

Znormalizowane współrzędne $(x, y)$ oraz współrzędne pikselowe $(u, v)$ służą do różnych celów, ale są ze sobą ściśle powiązane. Współrzędne znormalizowane są używane wszędzie tam, gdzie istotna jest struktura geometryczna sceny i relacje przestrzenne – np. w algorytmach rekonstrukcji 3D, lokalizacji kamery czy kalibracji. Z kolei współrzędne pikselowe są wykorzystywane do interakcji z obrazem: lokalizacji punktów na zdjęciu, wizualizacji, wykrywania cech czy ekstrakcji informacji wizualnych.

\chapter{Stereowizja}

Stereowizja, nazywana również wizją stereoskopową, jest techniką pozyskiwania informacji przestrzennych na podstawie dwóch lub więcej obrazów tej samej sceny, zarejestrowanych z różnych punktów obserwacyjnych. Metoda ta, inspirowana sposobem postrzegania przestrzeni przez ludzkie oczy, stanowi jedno z najstarszych i najbardziej rozwiniętych podejść do estymacji głębi w przetwarzaniu obrazu.

W przeciwieństwie do wizji monokularnej, stereowizja pozwala na geometrycznie uzasadnione, bezpośrednie obliczanie odległości do obiektów. Dzięki temu znajduje zastosowanie w systemach wymagających wysokiej precyzji pomiarów, takich jak robotyka, pojazdy autonomiczne czy systemy pomiarowe 3D.

W systemie stereowizyjnym wykorzystuje się dwa obrazy uzyskane przez kamery rozmieszczone w znanej odległości od siebie, określanej jako \textit{baza stereo}. Kluczowym pojęciem jest \textit{paralaksa} – przesunięcie położenia obrazu tego samego punktu sceny między lewym a prawym obrazem.

\section{Kalibracja}

Proces kalibracji kamery polega na wyznaczeniu jej parametrów wewnętrznych i zewnętrznych, co jest możliwe dzięki analizie serii zdjęć wzorca kalibracyjnego - najczęściej szachownicy — wykonanych pod różnymi kątami. Kluczowe jest, aby narożniki szachownicy były dobrze widoczne i możliwe do jednoznacznego zidentyfikowania przy użyciu funkcji \texttt{cv2.findChessboardCorners()}. Zgodnie z zaleceniami OpenCV, do uzyskania wiarygodnej kalibracji wymaganych jest co najmniej 10 obrazów dla każdej kamery. W niniejszym projekcie wykorzystano 32 obrazy kalibracyjne przypisane osobno do lewej i prawej kamery.

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.8\textwidth]{images/calib.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Widok wykrytych wierzchołków szachownic. https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/.]{Widok wykrytych wierzchołków szachownic.}
\end{figure}

Po wykryciu narożników ich pozycje są precyzowane za pomocą funkcji \texttt{cv2.cornerSubPix()}, co pozwala uzyskać większą dokładność w procesie kalibracji. Dane te są następnie zapisywane w postaci trzech wektorów:

\begin{itemize}
  \item imgpointsR: zawiera współrzędne narożników na prawym obrazie (w przestrzeni obrazu)
  \item imgpointsL: zawiera współrzędne narożników na lewym obrazie (w przestrzeni obrazu)
  \item objpoints: zawiera współrzędne narożników w przestrzeni obiektu.
\end{itemize}

Dane te wykorzystywane są przez funkcję \texttt{cv2.calibrateCamera()} do wyznaczenia macierzy kamery, współczynników dystorsji oraz wektorów rotacji i translacji dla każdej kamery. Macierz wewnętrzna kamery $\mathbf{M}$ opisuje rzutowanie punktów ze świata 3D na obraz 2D i przyjmuje postać:

\[
\mathbf{M} =
\begin{bmatrix}
f_x & 0 & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}
\]

\begin{itemize}
  \item $f_x = f \cdot s_x$ — ogniskowa wyrażona w pikselach w poziomie (oś X)
  \item $f_y = f \cdot s_y$ — ogniskowa wyrażona w pikselach w pionie (oś Y)
  \item $c_x$, $c_y$ — współrzędne punktu głównego (\textit{principal point}), zazwyczaj w centrum obrazu
  \item Zera poza przekątną oznaczają brak nachylenia między osiami (czyli brak efektu trapezowego)
  \item Wartość $1$ w prawym dolnym rogu służy do zapewnienia jednorodności w transformacjach macierzowych
\end{itemize}

Macierze kamer lewej i prawej uzyskane po kalibracji przedstawiają się następująco:

\[
\mathbf{M}_L =
\begin{bmatrix}
777,7 & 0 & 345,1 \\
0 & 780,6 & 171,3 \\
0 & 0 & 1 \\
\end{bmatrix}
\qquad
\mathbf{M}_R =
\begin{bmatrix}
792,2 & 0 & 274,9 \\
0 & 801,4 & 212,9 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

W celu dalszego przetwarzania obrazów i uzyskania lepszej dokładności, macierze te mogą zostać zoptymalizowane przy użyciu funkcji cv2.getOptimalNewCameraMatrix(). Zoptymalizowane wersje macierzy, uwzględniające rzeczywisty obszar widzenia i dystorsje, są wykorzystywane podczas rektyfikacji obrazów za pomocą funkcji cv2.stereoRectify().

Macierze kamer lewej i prawej uzyskane po rektyfikacji przedstawiają się następująco:

\[
\mathbf{M}_L =
\begin{bmatrix}
636,0 & 0 & 399,6 \\
0 & 760,1 & 170,2 \\
0 & 0 & 1 \\
\end{bmatrix}
\qquad
\mathbf{M}_R =
\begin{bmatrix}
772,2 & 0 & 284,1 \\
0 & 755,5 & 217,8 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

Aby wyznaczyć wzajemne położenie kamer względem siebie (rotację i translację), wykorzystywana jest funkcja cv2.stereoCalibrate(), która pozwala określić pełną konfigurację geometryczną układu stereo. Parametry te są niezbędne do poprawnego przekształcenia obrazów oraz dalszej analizy głębi, np. przy obliczaniu mapy dysparycji.

\section{Rektyfikacja stereo}

Jednym z kluczowych zagadnień w stereowizji jest \textbf{geometria epipolarna}, która opisuje zależność pomiędzy rzutami punktów przestrzennych na dwa obrazy uzyskane z różnych punktów widzenia. Celem tej geometrii jest ograniczenie przestrzeni poszukiwania punktów odpowiadających w drugim obrazie, co znacząco upraszcza dopasowywanie i rekonstrukcję głębi.

Aby dodatkowo uprościć ten proces, stosuje się \textbf{rektyfikację stereo}, czyli transformację obrazów, która sprowadza linie epipolarne do postaci równoległych i poziomych. Dzięki temu dopasowywanie punktów może być wykonywane wzdłuż jednej osi (najczęściej poziomej), co przyspiesza obliczenia i zwiększa ich dokładność.

\subsection{Geometria epipolarna}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/epipolar.png}
    \captionsetup{font=footnotesize}
    \caption[Model kamery stereo. Learning OpenCV 3, O'Reilly, Str. 709]{Model kamery stereo.}
    \label{fig:rpi}
\end{figure}

Na rysunku powyżej przedstawiono uproszczony model kamery stereo zbudowanej z dwóch kamer otworkowych. Punkty $O_l$ i $O_r$ to środki rzutów lewej i prawej kamery. Proste łączące punkty $p_l$ z $e_l$ oraz $p_r$ z $e_r$ to tzw. \textbf{linie epipolarne}, natomiast punkty $e_l$ i $e_r$ to \textbf{epipole} – rzuty środka jednej kamery na płaszczyznę obrazu drugiej.

Geometria epipolarna umożliwia ograniczenie przestrzeni przeszukiwania punktu odpowiadającego z pełnej płaszczyzny obrazu do jednej linii – linii epipolarnej. Ułatwia to znacznie proces dopasowywania punktów. Można to podsumować w następujących punktach:

\begin{itemize}
    \item Każdy punkt przestrzenny należy do płaszczyzny epipolarnej.
    \item Odpowiadający mu punkt w drugim obrazie musi leżeć na odpowiedniej linii epipolarnej (warunek epipolarny).
    \item Proces wyszukiwania punktu korespondującego można zredukować do jednego wymiaru, jeżeli znana jest geometria epipolarna.
    \item Kolejność punktów na liniach epipolarnych jest zachowana między obrazami.
\end{itemize}

\subsection{Macierz fundamentalna i esencjalna}

Aby matematycznie opisać zależności pomiędzy punktami w dwóch obrazach, wykorzystuje się dwie macierze: \textbf{macierz esencjalną} $E$ oraz \textbf{macierz fundamentalną} $F$. Macierz $E$ opisuje wzajemną orientację kamer (rotację i translację), natomiast $F$ uwzględnia dodatkowo parametry wewnętrzne kamer, takie jak ogniskowa czy środek obrazu.

Związek pomiędzy punktami $p_l$ i $p_r$ w obrazach opisuje równanie epipolarne:
\[
p_r^T E p_l = 0
\]

Ponieważ macierz $E$ ma rangę 2, opisuje ona jedynie płaszczyznę, nie zaś pełną transformację punktów. Dlatego wprowadza się macierz $F$, którą oblicza się jako:
\[
F = (M_r^{-1})^T E M_l^{-1}
\]
gdzie $M_l$ i $M_r$ to macierze wewnętrzne lewej i prawej kamery, a $q = M p$ to przekształcenie punktu przestrzennego do przestrzeni obrazu. Zatem pełna forma równania epipolarnego to:
\[
q_r^T F q_l = 0
\]

\subsection{Macierz obrotu i wektor translacji}

Aby wyznaczyć relację przestrzenną między kamerami, definiuje się:

\begin{itemize}
    \item $P_l = R_l P + T_l$ – przekształcenie punktu przestrzennego $P$ do układu lewej kamery,
    \item $P_r = R_r P + T_r$ – analogiczne przekształcenie do układu prawej kamery.
\end{itemize}

Na tej podstawie można wyznaczyć:
\[
P_l = R(P_r - T)
\]

co prowadzi do zależności:
\[
R = R_r R_l^T, \quad T = T_r - R T_l
\]

Celem rektyfikacji jest takie przekształcenie obrazów, aby ich linie epipolarne były współliniowe i poziome. W praktyce oznacza to sprowadzenie układów optycznych obu kamer do tej samej płaszczyzny rzutowania.

W wyniku rektyfikacji uzyskuje się dla każdej kamery:

\begin{itemize}
    \item wektor dystorsji,
    \item macierz rotacji rektyfikującej $R^{\text{RECT}}$,
    \item zrektyfikowaną macierz projekcji $M^{\text{RECT}}$,
    \item oryginalną (niezrektyfikowaną) macierz kamery $M$.
\end{itemize}

\subsection{Algorytm Hartley’a}

Algorytm Hartley’a \cite{hartley} pozwala przeprowadzić rektyfikację obrazów bez znajomości parametrów wewnętrznych kamer (metoda niekalibrowana).

\textbf{Etapy działania:}
\begin{itemize}
    \item Wyszukiwanie punktów korespondencyjnych (np. z użyciem cech SIFT, SURF, ORB),
    \item Estymacja macierzy fundamentalnej $F$,
    \item Obliczenie homografii, które przekształcają epipole do nieskończoności (linie epipolarne stają się poziome).
\end{itemize}

Wadą tej metody jest brak informacji o rzeczywistej skali sceny – relacje przestrzenne są wyłącznie względne.

\subsection{Algorytm Bouguet’a}

Algorytm Bouguet’a \cite{bouget}, stosowany m.in. w narzędziu \textit{Camera Calibration Toolbox} dla MATLAB-a, opiera się na wcześniejszej kalibracji kamer (metoda kalibrowana).

\textbf{Etapy działania:}
\begin{itemize}
    \item Kalibracja kamer z wykorzystaniem wzorca (np. szachownicy),
    \item Wyznaczenie parametrów wewnętrznych i zewnętrznych (R i T),
    \item Przekształcenie obrazów tak, aby ich osie optyczne były równoległe,
    \item Wygenerowanie zrektyfikowanych obrazów, w których odpowiadające piksele leżą na tych samych liniach poziomych.
\end{itemize}

Metoda ta zapewnia większą precyzję, lecz jest wrażliwa na błędy kalibracji (np. niedokładne wykrycie wzorca).

\[
\mathbf{M}_L^\text{RECT} =
\begin{bmatrix}
791 & 0 & 390.6 \\
0 & 791 & 194.4 \\
0 & 0 & 1 \\
\end{bmatrix}
\qquad
\mathbf{M}_R^\text{RECT} =
\begin{bmatrix}
791 & 0 & 390.6 \\
0 & 791 & 194.4 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

Tak przygotowane obrazy mogą być bezpośrednio wykorzystane do obliczania mapy dysparycji oraz rekonstrukcji 3D.

\section{Mapa dysparycji}

Mapa dysparycji \cite{disparity} to obraz, w którym każdemu pikselowi przypisywana jest wartość reprezentująca przesunięcie (różnicę pozycji) pomiędzy odpowiadającymi sobie punktami w obrazie lewym i prawym. Im większe przesunięcie (czyli dysparycja), tym bliżej znajduje się dany obiekt względem kamery. Taka mapa stanowi podstawę do obliczenia mapy głębokości, która pozwala oszacować odległość poszczególnych punktów sceny od obserwator.

\subsection{Algorytm StereoSGBM}

Jeden z algorytmów do obliczenia mapy dysparycji w bibliotece OpenCV jest \texttt{StereoSGBM}. Algorytm ten implementuje metodę półglobalnego dopasowania blokowego (ang. \textit{Semi-Global Block Matching}) \cite{semi-global}, która umożliwia estymację przesunięć pomiędzy odpowiadającymi sobie punktami w obrazach stereo — pochodzących z lewej i prawej kamery.

Podstawową ideą algorytmu jest porównywanie fragmentów (bloków) obrazów wzdłuż odpowiadających sobie linii epipolarnych, w celu znalezienia najlepszego dopasowania. Jednym z kluczowych parametrów wejściowych jest rozmiar bloku (\textit{block size}), który określa wielkość lokalnego sąsiedztwa uwzględnianego podczas dopasowania. Dla wartości większych niż 1, algorytm operuje na blokach pikseli, a nie na pojedynczych pikselach, co pozwala zwiększyć odporność na szum kosztem precyzji w obszarach o drobnych szczegółach.

Zakładając, że kalibracja i rektyfikacja zostały przeprowadzone prawidłowo, dopasowanie odbywa się wzdłuż linii epipolarnych, które w zrektyfikowanych obrazach pokrywają się z liniami poziomymi. Dzięki temu proces wyszukiwania korespondencji ogranicza się do jednej osi (poziomej), co znacząco redukuje złożoność obliczeniową. Przykładowo, blok o współrzędnych $(4,7)$ w obrazie referencyjnym (np. lewym) porównywany jest z blokami w tej samej linii poziomej obrazu dopasowywanego (np. prawego), tj. $(4,i)$.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/dopracy1.png}
\captionsetup{font=footnotesize}
\caption[Wyszukiwanie pasujących bloków za pomocą StereoSGBM. Opracowanie własne.]{Wyszukiwanie pasujących bloków za pomocą StereoSGBM.}
\end{figure}

Dopasowanie oceniane jest na podstawie funkcji kosztu, a im mniejsza wartość kosztu, tym większe prawdopodobieństwo, że porównywane bloki odpowiadają temu samemu punktowi w przestrzeni trójwymiarowej. W przedstawionym przykładzie najwyższe dopasowanie dla bloku $(4,7)$ uzyskano względem bloku $(4,4)$ w obrazie prawym.

W przeciwieństwie do metod opierających się wyłącznie na lokalnych dopasowaniach, algorytm \texttt{StereoSGBM} minimalizuje energię globalną, uwzględniając dodatkowe kierunki propagacji dopasowania. W implementacji OpenCV analizowanych jest pięć kierunków, co pozwala ograniczyć ryzyko lokalnych błędów dopasowania i poprawia spójność wyników.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/dopracy2.png}
\captionsetup{font=footnotesize}
\caption[Pięć kierunków przeszukiwania w algorytmie StereoSGBM. Opracowanie własne.]{Pięć kierunków przeszukiwania w algorytmie StereoSGBM.}
\end{figure}

Wartość dysparycji obliczana jest jako różnica współrzędnych poziomych piksela (lub bloku) w obrazie lewym i odpowiadającej mu pozycji w obrazie prawym. Zgodnie z zasadą triangulacji, większa wartość dysparycji oznacza, że obiekt znajduje się bliżej kamery. W praktyce przyjmuje się wartość bezwzględną tej różnicy.

Algorytm operuje zazwyczaj na obrazach w odcieniach szarości, co znacząco zmniejsza czas obliczeń. Możliwe jest użycie obrazów kolorowych, jednak wiąże się to ze zwiększonym zapotrzebowaniem na moc obliczeniową, bez istotnej poprawy jakości wyników.

Po skonfigurowaniu obiektu \texttt{StereoSGBM}, obliczenie mapy dysparycji następuje poprzez wywołanie metody \texttt{compute()}. Wynikowy obraz przedstawia poziomą różnicę położenia punktów w obrazach stereo, co stanowi podstawę do dalszej rekonstrukcji głębi.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/disparity.png}
\captionsetup{font=footnotesize}
\caption[Rezultat mapy dysparycji StereoSGBM. Opracowanie własne.]{Rezultat mapy dysparycji StereoSGBM.}
\end{figure}
\subsection{Algorytm StereoBM}

Alternatywą dla metody \texttt{StereoSGBM} jest algorytm \texttt{StereoBM} (ang. \textit{Block Matching}) \cite{bm}, będący jedną z najstarszych i najprostszych implementacji dopasowania stereoskopowego w bibliotece OpenCV. Jego działanie opiera się na zasadzie bezpośredniego porównywania lokalnych bloków pikseli pomiędzy obrazem lewym i prawym wzdłuż odpowiadających sobie linii epipolarnych. \texttt{StereoBM} nie wykorzystuje bezpośrednio parametrów kalibracyjnych w procesie dopasowania – działa tylko na rektyfikowanych obrazach. 

Proces dopasowania rozpoczyna się od wyboru bloku referencyjnego w obrazie lewym. Dla każdego takiego bloku algorytm przeszukuje wzdłuż tej samej linii epipolarnej w obrazie prawym obszar o zadanym zakresie dysparycji, aby znaleźć najbardziej podobny fragment. Do oceny podobieństwa stosowana jest prosta funkcja kosztu, najczęściej \textbf{Sum of Absolute Differences (SAD)}, definiowana jako:

\[
\text{SAD}(x,y,d) = \sum_{i=-\frac{w}{2}}^{\frac{w}{2}} \sum_{j=-\frac{h}{2}}^{\frac{h}{2}} | I_L(x+i,y+j) - I_R(x+i-d,y+j) |
\]

gdzie:
\begin{itemize}
    \item $(x,y)$ -- współrzędne środka bloku w obrazie lewym,
    \item $d$ -- wartość przesunięcia (kandydat na dysparycję),
    \item $w,h$ -- szerokość i wysokość bloku dopasowania,
    \item $I_L$, $I_R$ -- intensywności pikseli w obrazie lewym i prawym.
\end{itemize}

Dysparycja dla danego punktu wybierana jest jako wartość $d$, dla której funkcja kosztu osiąga minimum.  

Algorytm ten analizuje wyłącznie dopasowania lokalne, bez uwzględnienia kontekstu globalnego, co odróżnia go od metody \texttt{StereoSGBM}, która implementuje optymalizację w wielu kierunkach i minimalizuje energię globalną. Brak optymalizacji globalnej skutkuje mniejszą dokładnością w obszarach jednorodnych (bez wyraźnej tekstury), w cieniu oraz przy obecności szumów.  

\textbf{Najważniejsze różnice między \texttt{StereoBM} a \texttt{StereoSGBM}:}
\begin{itemize}
    \item \textbf{Metoda dopasowania:}  
    \texttt{StereoBM} stosuje lokalne dopasowanie blokowe na podstawie funkcji SAD, natomiast \texttt{StereoSGBM} rozszerza dopasowanie o koszt gradientu intensywności i optymalizację półglobalną.
    
    \item \textbf{Kontekst globalny:}  
    \texttt{StereoBM} ignoruje zależności przestrzenne między sąsiadującymi pikselami, podczas gdy \texttt{StereoSGBM} propaguje koszty w wielu kierunkach, co redukuje błędy dopasowania w jednorodnych obszarach.
    
    \item \textbf{Jakość mapy dysparycji:}  
    Wyniki \texttt{StereoBM} są zazwyczaj gorsze w obszarach o niskim kontraście i w cieniu. \texttt{StereoSGBM} lepiej radzi sobie z tymi przypadkami dzięki dodatkowym ograniczeniom ciągłości dysparycji.
    
    \item \textbf{Złożoność obliczeniowa:}  
    \texttt{StereoBM} jest znacząco szybszy i mniej zasobożerny, co czyni go atrakcyjnym rozwiązaniem w systemach o ograniczonej mocy obliczeniowej, natomiast \texttt{StereoSGBM} wymaga więcej pamięci i czasu obliczeń.
\end{itemize}

Ze względu na prostotę algorytmu i brak optymalizacji globalnej, wyniki uzyskane przy użyciu \texttt{StereoBM} są bardziej podatne na błędy dopasowania (tzw. szumy dysparycji), szczególnie w obszarach pozbawionych tekstury lub przy zakłóceniach oświetlenia.

\texttt{StereoBM} jest powszechnie stosowany w systemach wymagających przetwarzania w czasie rzeczywistym przy ograniczonych zasobach, np. w robotyce mobilnej lub systemach wizyjnych dla pojazdów autonomicznych, gdy wymagana jest szybka, ale przybliżona estymacja głębi.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{images/BMdisparity.png}
    \captionsetup{font=footnotesize}
    \caption[Rezultat mapy dysparycji StereoBM. Opracowanie własne.]{Rezultat mapy dysparycji uzyskanej metodą \texttt{StereoBM}.}
\end{figure}
\section{Filtr WLS}

W celu dalszego ograniczenia szumu w mapie dysparycji stosowany jest filtr ważonych najmniejszych kwadratów(ang. \textit{Weighted Least Squares}) \cite{wls}, dostępny w module \texttt{cv2.ximgproc}. Jest on szczególnie skuteczny w poprawianiu ciągłości strukturalnej oraz w zachowaniu ostrych krawędzi w scenach o złożonej geometrii.

Parametr lambda kontroluje stopień wygładzania mapy: im wyższa jego wartość, tym bardziej struktura wynikowej mapy przypomina obraz referencyjny (np. lewy obraz stereo). W praktyce często stosuje się wartości rzędu 8000, jednak w omawianym przypadku zastosowano wartość \texttt{lambda = 80000}, co pozwoliło uzyskać bardziej stabilne rezultaty. Z kolei parametr sigma określa, jak precyzyjnie filtr ma traktować obszary znajdujące się w pobliżu krawędzi – wyższe wartości sprzyjają lepszemu zachowaniu konturów obiektów.

Aby skorzystać z filtra WLS, tworzony jest dodatkowy obiekt dopasowania stereo dla prawego obrazu, przy użyciu funkcji \texttt{cv2.ximgproc.createRightMatcher()}, bazujący na głównym obiekcie StereoSGBM. Następnie inicjalizowany jest filtr WLS poprzez \texttt{cv2.ximgproc.\allowbreak createDisparityWLSFilter()} i konfigurowany z użyciem uprzednio utworzonych dopasowań.

Sam proces filtracji przeprowadzany jest metodą \texttt{filter()} filtra WLS, a jego wynik poddawany jest normalizacji za pomocą funkcji \texttt{cv2.normalize()}, co pozwala na wizualizację rezultatu.

Należy jednak zauważyć, że wynik filtru WLS nie jest już bezpośrednio wykorzystywalną mapą dysparycji — jest to obraz zakodowany w formacie uint8, który dobrze uwidacznia krawędzie, lecz nie zawiera już rzeczywistych wartości głębokości (w przeciwieństwie do oryginalnej mapy dysparycji w formacie float32).

Tak wygląda rezultat filtra WLS:

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.8\textwidth]{images/combinedWLS.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Rezultat filtra WLS dla StereoSGBM i StereoBM. Opracowanie własne.]{Rezultat filtra WLS dla StereoSGBM i StereoBM.}
\end{figure}

\subsection{Filtr zamykający}

Na uzyskanym obrazie mogą nadal występować zakłócenia w postaci lokalnych szumów. W celu ich redukcji stosuje się filtrację morfologiczną, która pozwala poprawić spójność i ciągłość danych głębokości. W szczególności wykorzystywany jest operator zamykania (morphological closing), realizowany w OpenCV za pomocą funkcji \texttt{cv2.morphologyEx()} z flagą \texttt{cv2.MORPH\_CLOSE}. Zastosowanie tego filtru pozwala na eliminację drobnych czarnych artefaktów, które mogą pojawić się wewnątrz jednorodnych obszarów na mapie.

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.6\textwidth]{images/closeF.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Przykład filtra zamykającego. https://www.graphicsmill.com/docs/gm/minimum-maximum-median-filters.htm]{Przykład filtra zamykającego.}
\end{figure}

\section{Triangulacja}

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.5\textwidth]{images/triangulation.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Schemat triangulacji. Learning OpenCV 3, O'Reilly, Str. 705]{Schemat triangulacji.}
    \label{fig:rpi}  % Optional: use to refer to this image later in the text
\end{figure}

W ostatnim kroku, triangulacji, zakłada się, że oba obrazy projekcji są współpłaszczyznowe i że poziomy rząd pikseli lewego obrazu jest wyrównany z odpowiadającym mu obrazem prawego.

Punkt $P$ leży w środowisku i jest pokazany na
lewym i prawym obrazie na $P_l$ i $P_r$, z odpowiadającymi im współrzędnymi
odpowiadającymi współrzędnymi $x^l$ i $x^r$. To pozwala nam wprowadzić nową wielkość $d = x^l - x^r$.
Można zauważyć, że im dalej punkt $P$, tym mniejsza staje się wielkość $d$. Dysproporcja jest zatem odwrotnie proporcjonalna do odległości.\\
Do obliczenia odległości można użyć następującego wzoru można obliczyć: \[Z=f*T/(x^l-x^r)\]

Można zauważyć, że istnieje nieliniowa zależność między rozbieżnością, a odległością.
Jeśli rozbieżność jest bliska 0, małe różnice w rozbieżności prowadzą do dużych różnic w odległości.
Zjawisko to ulega odwróceniu, gdy rozbieżność jest duża. Małe różnice dysproporcji nie prowadzą do dużych różnic odległości. Na tej podstawie można wywnioskować, że stereowizja ma wysoką rozdzielczość głębi, tylko dla obiektów znajdujących się blisko kamery.

\chapter{Funkcjonalność projektu}

\begin{lstlisting}[style=mypython]
# Termination criteria
term_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
\end{lstlisting}

Na początku definiowane są kryteria zakończenia algorytmu optymalizacji (term\_criteria), które łączą dwa warunki:

\begin{itemize}
    \item osiągnięcie maksymalnej liczby iteracji (30),
    \item osiągnięcie dokładności wyrażonej w zmianie pozycji punktu poniżej zadanego progu (0.001).
\end{itemize}

Kryteria te są następnie wykorzystywane w procedurze doprecyzowywania lokalizacji narożników szachownicy.

\begin{lstlisting}[style=mypython]
# Prepare object points
objp = np.zeros((9*6,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)
\end{lstlisting}

Tworzona jest macierz punktów obiektowych (objp), która reprezentuje idealny, znany geometrycznie układ narożników szachownicy w przestrzeni 3D. Przyjęto, że szachownica ma wymiar 9 × 6 pól (tj. 54 narożniki). Punkty są rozmieszczone w płaszczyźnie Z = 0, dzięki czemu uzyskano dwuwymiarową siatkę punktów w przestrzeni obiektowej

\begin{lstlisting}[style=mypython]
# Arrays to store object points and image points from all images
objpoints= []   # 3d points in real world space
imgpointsR= []   # 2d points in image plane
imgpointsL= []

ChessImaR = None
ChessImaL = None

for i in range(0, 64):
    t = str(i)
    ChessImaR = cv2.imread('calib_images/right_chessboard-' + t + '.png', 0)
    ChessImaL = cv2.imread('calib_images/left_chessboard-' + t + '.png', 0)
    if ChessImaR is None or ChessImaL is None:
        continue  # Skip this iteration if loading failed
    retR, cornersR = cv2.findChessboardCorners(ChessImaR, (9, 6), None)
    retL, cornersL = cv2.findChessboardCorners(ChessImaL, (9, 6), None)
    if retR and retL:
        objpoints.append(objp)
        cv2.cornerSubPix(ChessImaR, cornersR, (11, 11), (-1, -1), term_criteria)
        cv2.cornerSubPix(ChessImaL, cornersL, (11, 11), (-1, -1), term_criteria)
        imgpointsR.append(cornersR)
        imgpointsL.append(cornersL)
\end{lstlisting}

Program iteruje przez zestaw obrazów kalibracyjnych (64 par zdjęć lewej i prawej kamery). Dla każdej pary:

\begin{itemize}
    \item wczytywane są obrazy w odcieniach szarości,
    \item wykonywana jest detekcja narożników szachownicy (cv2.findChessboardCorners),
    \item w przypadku sukcesu, pozycje narożników są doprecyzowywane metodą subpikselową (cv2.cornerSubPix),
    \item wykonywana jest detekcja narożników szachownicy (cv2.findChessboardCorners),
    \item współrzędne narożników (punkty obrazu) dodawane są do listy punktów obrazowych dla każdej kamery (imgpointsL, imgpointsR), przy jednoczesnym przypisaniu do odpowiadających im punktów obiektowych (objpoints).
\end{itemize}

\begin{lstlisting}[style=mypython]
# Calibration
retR, mtxR, distR, rvecsR, tvecsR = cv2.calibrateCamera(objpoints, imgpointsR, ChessImaR.shape[::-1], None, None)
retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(objpoints, imgpointsL, ChessImaL.shape[::-1], None, None)
\end{lstlisting}

Dla każdej z kamer przeprowadzana jest osobna kalibracja przy użyciu funkcji cv2.calibrateCamera. Wynikiem są:

\begin{itemize}
    \item macierze wewnętrzne kamer (mtxL, mtxR),
    \item współczynniki dystorsji (distL, distR),
    \item parametry związane z położeniem i orientacją kamery dla każdej pozycji wzorca (rvecs, tvecs).
\end{itemize}

\begin{lstlisting}[style=mypython]
OmtxR, roiR = cv2.getOptimalNewCameraMatrix(mtxR, distR, ChessImaR.shape[::-1], 1, ChessImaR.shape[::-1])
OmtxL, roiL = cv2.getOptimalNewCameraMatrix(mtxL, distL, ChessImaL.shape[::-1], 1, ChessImaL.shape[::-1])
\end{lstlisting}

Następnie obliczane są optymalne macierze rzutowania (cv2.getOptimalNewCameraMatrix), które minimalizują wpływ dystorsji przy jednoczesnym zachowaniu maksymalnego obszaru obrazu.

\begin{lstlisting}[style=mypython]
OmtxR, roiR = cv2.getOptimalNewCameraMatrix(mtxR, distR, ChessImaR.shape[::-1], 1, ChessImaR.shape[::-1])
OmtxL, roiL = cv2.getOptimalNewCameraMatrix(mtxL, distL, ChessImaL.shape[::-1], 1, ChessImaL.shape[::-1])
\end{lstlisting}

\begin{lstlisting}[style=mypython]
retS, MLS, dLS, MRS, dRS, R, T, E, F= cv2.stereoCalibrate(objpoints,imgpointsL,imgpointsR,mtxL,distL,mtxR,distR,ChessImaR.shape[::-1],criteria = term_criteria,flags = cv2.CALIB_FIX_INTRINSIC)
\end{lstlisting}

W ostatnim etapie wykonywana jest właściwa stereokalibracja przy użyciu funkcji cv2.stereoCalibrate.
Proces ten, przy założeniu stałych parametrów wewnętrznych kamer (cv2.CALIB\_FIX\_INTRINSIC), estymuje:

\begin{itemize}
    \item macierz rotacji R,
    \item wektor translacji T,
    \item macierz fundamentalną F,
    \item macierz epipolarną E.
\end{itemize}

Parametry te określają wzajemne położenie i orientację obu kamer, a także zależności geometryczne między punktami obrazu zarejestrowanymi przez lewą i prawą kamerę.

\begin{lstlisting}[style=mypython]
rectify_scale= 0 # if 0 image croped, if 1 image nor croped
RL, RR, PL, PR, Q, roiL, roiR= cv2.stereoRectify(MLS, dLS, MRS, dRS, ChessImaR.shape[::-1], R, T, rectify_scale,(0,0))  # last paramater is alpha, if 0= croped, if 1= not croped
\end{lstlisting}

Powyższy fragment kodu implementuje \textbf{algorytm Bougueta} (rektyfikacja kalibrowana), który wykorzystuje pełny zestaw parametrów kalibracyjnych kamer do wyznaczenia rzutów obrazów na wspólną płaszczyznę epipolarną. 
Funkcja \texttt{cv2.stereoRectify} korzysta z macierzy wewnętrznych kamer ($M_{LS}, M_{RS}$), współczynników dystorsji ($d_{LS}, d_{RS}$), a także parametrów opisujących wzajemne położenie kamer w przestrzeni ($R,T$). 
Na tej podstawie estymowane są:

\begin{itemize}
    \item macierze rektyfikacji ($R_L,R_R$) dla lewej i prawej kamery,
    \item nowe macierze rzutowania ($P_L,P_R$),
    \item macierz dysparycji do głębi $Q$, umożliwiająca odwzorowanie punktów obrazu w trójwymiarową przestrzeń,
    \item regiony zainteresowania (ROI), określające użyteczny obszar obrazu po rektyfikacji.
\end{itemize}

Parametr \texttt{rectify\_scale} definiuje stopień przycięcia obrazu:

\begin{itemize}
    \item $\alpha = 0$ – obrazy są przycięte w celu wyeliminowania obszarów pozbawionych informacji,
    \item $\alpha = 1$ – obrazy nie są przycinane, co pozwala zachować pełne pole widzenia, kosztem obecności obszarów pustych.
\end{itemize}

\begin{lstlisting}[style=mypython]
Left_Stereo_Map= cv2.initUndistortRectifyMap(MLS, dLS, RL, PL, ChessImaR.shape[::-1], cv2.CV_16SC2)   # cv2.CV_16SC2 this format enables us the programme to work faster
Right_Stereo_Map= cv2.initUndistortRectifyMap(MRS, dRS, RR, PR, ChessImaR.shape[::-1], cv2.CV_16SC2)
\end{lstlisting}

Następnie, przy pomocy funkcji \texttt{cv2.initUndistortRectifyMap}, obliczane są mapy przekształceń dla lewej i prawej kamery. 
Mapy te definiują sposób, w jaki każdy piksel obrazu wejściowego powinien zostać przesunięty w celu eliminacji zniekształceń soczewki oraz dostosowania do nowego układu rektyfikowanego.

W kodzie generowane są dwie mapy:

\begin{itemize}
    \item \texttt{Left\_Stereo\_Map} dla obrazu lewego,
    \item \texttt{Right\_Stereo\_Map} dla obrazu prawego.
\end{itemize}

Dane wyjściowe mają format \texttt{cv2.CV\_16SC2}, co pozwala na szybsze działanie programu dzięki wykorzystaniu mapy przesunięć w formie stałoprzecinkowej.

Algorytm Hartleya \cite{hartley} umożliwia rektyfikację obrazów stereo w przypadku braku wcześniejszej kalibracji układu kamer. W odróżnieniu od metody opartej na \texttt{stereoRectify}, która wymaga znajomości parametrów wewnętrznych i zewnętrznych kamer, tutaj proces bazuje wyłącznie na dopasowanych punktach korespondencyjnych pomiędzy obrazami lewym i prawym.

\textbf{Wykrycie i opis cech lokalnych} – w obu obrazach (lewym i prawym) detekcja cech przeprowadzana jest za pomocą algorytmu ORB, który zwraca punkty kluczowe oraz deskryptory opisujące ich lokalny kontekst.

\begin{lstlisting}[style=mypython]
# Wykrycie i opis cech (np. ORB)
orb = cv2.ORB_create()
kp1, des1 = orb.detectAndCompute(imgL, None)
kp2, des2 = orb.detectAndCompute(imgR, None)
\end{lstlisting}

\textbf{Dopasowanie punktów korespondencyjnych} – wykorzystując algorytm \texttt{BFMatcher} z metryką Hamminga, dopasowywane są deskryptory z obu obrazów. Wyniki sortowane są według odległości, co pozwala wybrać najlepiej dopasowane pary punktów.

\begin{lstlisting}[style=mypython]
# Dopasowanie cech metodą BFMatcher
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1, des2)
matches = sorted(matches, key=lambda x: x.distance)
\end{lstlisting}

\textbf{Estymacja macierzy fundamentalnej} – na podstawie uzyskanych dopasowań estymowana jest macierz fundamentalna $F$, opisująca geometryczne powiązanie pomiędzy obrazami stereo. Wykorzystanie metody \texttt{RANSAC} zapewnia odporność na błędne dopasowania (outliery).

\begin{lstlisting}[style=mypython]
# Konwersja dopasowań do tablic punktów
pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)
pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)
\end{lstlisting}

\textbf{Filtracja inlierów} – do dalszych obliczeń wykorzystywane są jedynie punkty spełniające model geometryczny wynikający z macierzy fundamentalnej.

\begin{lstlisting}[style=mypython]
# Estymacja macierzy fundamentalnej
F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)
\end{lstlisting}

\textbf{Obliczenie homografii rektyfikujących} – funkcja \texttt{stereoRectifyUncalibrated} wyznacza transformacje homograficzne $H_1$ i $H_2$, które przekształcają obrazy tak, aby epipolarne linie były wyrównane wzdłuż osi poziomej.

\begin{lstlisting}[style=mypython]
# Wybór tylko inlierów
pts1 = pts1[mask.ravel()==1]
pts2 = pts2[mask.ravel()==1]
\end{lstlisting}

\textbf{Rektyfikacja obrazów} – poprzez zastosowanie przekształceń homograficznych ($H_1$ i $H_2$) uzyskuje się zrektyfikowane obrazy lewe i prawe, które mogą zostać użyte w dalszych etapach przetwarzania (np. obliczania mapy dysparycji).

\begin{lstlisting}[style=mypython]
# Obliczenie homografii rektyfikujących
ret, H1, H2 = cv2.stereoRectifyUncalibrated(
    pts1, pts2, F, imgSize=imgL.shape[::-1]
)
\end{lstlisting}

\begin{lstlisting}[style=mypython]
# Zastosowanie przekształceń homograficznych
imgL_rect = cv2.warpPerspective(imgL, H1, imgL.shape[::-1])
imgR_rect = cv2.warpPerspective(imgR, H2, imgR.shape[::-1])
\end{lstlisting}

Poniższy fragment kodu przedstawia proces inicjalizacji algorytmu \textit{Semi-Global Block Matching} (StereoSGBM) w bibliotece OpenCV.  
Algorytm ten jest wykorzystywany do estymacji mapy dysparycji na podstawie sparowanych obrazów stereo.  

\begin{lstlisting}[style=mypython]
# Inicjalizacja StereoSGBM
block_size = 7
min_disp = 2
num_disp = 130-min_disp
stereo = cv2.StereoSGBM_create(
    minDisparity = min_disp,
    numDisparities = num_disp,
    blockSize = block_size,
    uniquenessRatio = 10,
    speckleWindowSize = 100,
    speckleRange = 32,
    disp12MaxDiff = 5,
    P1 = 8*3*block_size**2,
    P2 = 32*3*block_size**2
)
\end{lstlisting}

Parametry algorytmu pełnią następujące funkcje:

\begin{itemize}
    \item \textbf{minDisparity} - minimalna oczekiwana wartość dysparycji. Określa najmniejsze przesunięcie pikseli między obrazami lewym i prawym. Zwykle jest równa 0 lub niewielkiej wartości dodatniej.
    \item \textbf{numDisparities} - liczba poziomów dysparycji, które będą analizowane. Musi być wielokrotnością 16. Im większa wartość, tym większy zakres głębi można zarejestrować, kosztem wydajności obliczeniowej.
    \item \textbf{blockSize} - rozmiar bloku (\textit{okna}), na podstawie którego porównywane są fragmenty obrazów. Duże wartości zwiększają odporność na szumy, ale pogarszają precyzję przy krawędziach obiektów.
    \item \textbf{uniquenessRatio} - minimalna różnica procentowa między najlepszym a drugim najlepszym dopasowaniem. Wyższe wartości pozwalają odrzucać niepewne wyniki.
    \item \textbf{speckleWindowSize} - maksymalny rozmiar obszaru złożonego z przypadkowych artefaktów (tzw. \textit{speckles}), który zostanie usunięty.
    \item \textbf{speckleRange} - maksymalna dozwolona różnica dysparycji w obrębie speckle. Parametr ten pomaga eliminować niespójne obszary.
    \item \textbf{disp12MaxDiff} - maksymalna dopuszczalna różnica pomiędzy wynikami uzyskanymi w dopasowaniu lewo-prawo oraz prawo-lewo. Ma na celu zapewnienie spójności obliczeń.
    \item \textbf{P1} - kara za niewielkie zmiany dysparycji między sąsiednimi pikselami. Wartość ta jest proporcjonalna do liczby kanałów obrazu oraz kwadratu wielkości bloku.
    \item \textbf{P2} - kara za większe zmiany dysparycji, znacząco wyższa niż \textbf{P1}. Zapewnia gładkość mapy dysparycji poprzez redukcję gwałtownych skoków wartości.
\end{itemize}

Poniższy fragment kodu ilustruje zastosowanie metody \textit{Block Matching} (StereoBM) do wyznaczania mapy dysparycji na podstawie obrazów stereo. Algorytm ten opiera się na lokalnym porównywaniu bloków pikseli pomiędzy lewym i prawym obrazem w celu oszacowania przesunięć odpowiadających sobie punktów.  

\begin{lstlisting}[style=mypython]
# Inicjalizacja StereoBM
num_disp = 64 # musi być podzielne przez 16
block_size = 15 # większy blok = większa odporność na szumy

stereo = cv2.StereoBM_create(numDisparities=num_disp,blockSize=block_size)
\end{lstlisting}

W powyższym kodzie najpierw inicjalizowany jest obiekt klasy \texttt{StereoBM}, przy czym parametr \texttt{numDisparities} określa maksymalny zakres przesunięć (dysparycji), a \texttt{blockSize} definiuje rozmiar lokalnego okna porównawczego. Większy rozmiar bloku zwiększa odporność na szumy, jednak odbywa się to kosztem precyzji na krawędziach i przy drobnych szczegółach obrazu.  

\begin{lstlisting}[style=mypython]
# Tworzenie dopasowania w kierunku prawym
stereoR = cv2.ximgproc.createRightMatcher(stereo)
\end{lstlisting}

Potrzebna jest druga instancja dopasowania (\texttt{stereoR}) z wykorzystaniem funkcji \texttt{cv2.ximgproc.createRightMatcher}. Jest to wariant algorytmu obliczający dopasowanie obrazu prawego względem lewego, co umożliwia późniejsze zastosowanie filtrów spójności. 

W celu poprawy jakości uzyskanej mapy dysparycji stosuje się filtrację post-procesową. W poniższym fragmencie kodu wykorzystano filtrację metodą \textit{Weighted Least Squares} (WLS), dostępną w module \texttt{cv2.ximgproc}. Technika ta redukuje artefakty i szumy w mapie dysparycji, zwłaszcza w rejonach o niskim kontraście i w pobliżu krawędzi obiektów.  

\begin{lstlisting}[style=mypython]
lmbda = 80000
sigma = 1.8
visual_multiplier = 1.0
wls_filter = cv2.ximgproc.createDisparityWLSFilter(matcher_left=stereo)
wls_filter.setLambda(lmbda)
wls_filter.setSigmaColor(sigma)
\end{lstlisting}

Parametr \texttt{lambda} (\( \lambda \)) kontroluje siłę wygładzania – większa wartość powoduje bardziej globalne uśrednianie mapy dysparycji, co redukuje zakłócenia, lecz może prowadzić do utraty szczegółów. Z kolei parametr \texttt{sigma} reguluje wpływ informacji barwnej podczas filtracji; większe wartości umożliwiają zachowanie ciągłości na jednolitych obszarach, a jednocześnie lepsze odwzorowanie krawędzi obiektów.  

Zmienna \texttt{visual\_multiplier} określa współczynnik skalujący, wykorzystywany wyłącznie do wizualizacji przefiltrowanej mapy, nie wpływając na same wyniki obliczeń.  

\begin{lstlisting}[style=mypython]
Cam = cv2.VideoCapture(0)
Cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1100)  
Cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 270)
Cam.set(cv2.CAP_PROP_FPS, 60)

executor = ThreadPoolExecutor(max_workers=4)

focal_length_px = PL[0, 0]
baseline_m = abs(T[0][0]) / 100  

# --- Parametry wykrywania objektów --- #
MIN_DISTANCE_TRIGGER = 0.65
MAX_DISTANCE_RELEASE = 0.7
DISPARITY_RANGE = (1.0, 150.0)
CONTOUR_AREA_THRESHOLD = 500
DISP_AVG_HISTORY = 5

distance_history = deque(maxlen=DISP_AVG_HISTORY)

while True:
    current_time = time.time()
    ret, frame = Cam.read()
    if not ret:
        print("Failed to capture frame. Exiting...")
        break

    height, width, _ = frame.shape
    mid = width // 2

    left_frame = frame[:, :mid]
    right_frame = frame[:, mid:]

    # --- Remap rectificated images --- #
    Left_nice = executor.submit(cv2.remap, left_frame, Left_Stereo_Map[0], Left_Stereo_Map[1],
                                interpolation=cv2.INTER_LANCZOS4, borderMode=cv2.BORDER_CONSTANT).result()
    Right_nice = executor.submit(cv2.remap, right_frame, Right_Stereo_Map[0], Right_Stereo_Map[1],
                                 interpolation=cv2.INTER_LANCZOS4, borderMode=cv2.BORDER_CONSTANT).result()

    # --- Convert to grayscale --- #
    gray_left = executor.submit(cv2.cvtColor, Left_nice, cv2.COLOR_BGR2GRAY).result()
    gray_right = executor.submit(cv2.cvtColor, Right_nice, cv2.COLOR_BGR2GRAY).result()

    # --- Compute disparity maps --- #
    futureL = executor.submit(lambda: stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0)
    futureR = executor.submit(lambda: stereoR.compute(gray_right, gray_left).astype(np.float32) / 16.0)
    dispL = futureL.result()
    dispR = futureR.result()

    # --- Apply WLS filter --- #
    filtered_disp = wls_filter.filter(dispL, gray_left, None, dispR)

    # --- Closing Filter --- #
    disp_closed = cv2.morphologyEx(filtered_disp, cv2.MORPH_CLOSE, kernel)

    disp_vis = cv2.normalize(disp_closed, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

    # --- ROI image to analyze --- #
    disp_height, disp_width = disp_vis.shape
    roi_width = disp_width // 3  # 1/3 width
    roi_height = disp_height // 2  # 1/2 height
    roi_x = (disp_width - roi_width + 150) // 2
    roi_y = (disp_height - roi_height) // 2
    roi_rect = (roi_x, roi_y, roi_width, roi_height)

    # --- Visualize ROI --- #
    cv2.rectangle(disp_closed, (roi_x, roi_y), (roi_x + roi_width, roi_y + roi_height), (0, 255, 255), 2)

    # Detect close objects via thresholding
    _, close_mask = cv2.threshold(disp_closed, 160, 255, cv2.THRESH_BINARY)

    # Restrict mask to ROI only
    roi_mask = np.zeros_like(close_mask)
    roi_mask[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width] = close_mask[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width]

    # Find contours in ROI restricted region
    contours, _ = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    current_stop_flag = False

    for cnt in contours:
        if cv2.contourArea(cnt) < CONTOUR_AREA_THRESHOLD:
            continue

        x, y, w, h = cv2.boundingRect(cnt)
        cx, cy = x + w // 2, y + h // 2

        # Make sure coordinates are within image bounds
        if cy - 1 < 0 or cy + 2 > dispL.shape[0] or cx - 1 < 0 or cx + 2 > dispL.shape[1]:
            continue

        roi_disp = dispL[cy - 1:cy + 2, cx - 1:cx + 2]
        valid_disp = roi_disp[(roi_disp > DISPARITY_RANGE[0]) & (roi_disp < DISPARITY_RANGE[1])]

        if valid_disp.size == 0:
            continue

        avg_disp = np.median(valid_disp)
        distance = (focal_length_px * baseline_m) / avg_disp
        distance_history.append(distance)
        smoothed_distance = np.median(distance_history)

        # Apply hysteresis logic for stability
        if smoothed_distance < MIN_DISTANCE_TRIGGER:
            current_stop_flag = True
            box_color = (0, 0, 255) if smoothed_distance < 0.5 else (0, 255, 0)
            cv2.rectangle(disp_closed, (x, y), (x + w, y + h), box_color, 2)
            cv2.putText(disp_closed, f"{smoothed_distance:.2f} m", (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)
            break

    # Trigger GPIO only once per frame
    # if current_stop_flag:
    #     lgpio.gpio_write(chip, PIN, 0)  # Stop
    # elif len(distance_history) == DISP_AVG_HISTORY and smoothed_distance > MAX_DISTANCE_RELEASE:
    #     lgpio.gpio_write(chip, PIN, 1)  # Move forward

\end{lstlisting}

\chapter{Wnioski i możliwe ulepszenia}

Z przeprowadzonej analizy uzyskanych obrazów można stwierdzić, że zastosowanie systemu antykolizyjnego opartego wyłącznie na analizie obrazu jest niewystarczające. 
Stereowizja posiada bowiem szereg ograniczeń, które wpływają na jej niezawodność, m.in.: 

\begin{itemize}
    \item problemy przy silnym oświetleniu oraz odbiciach, gdzie zmienność intensywności zakłóca proces dopasowania pikseli,
    \item brak możliwości dopasowania w obszarach ubogich w teksturę, takich jak białe ściany czy jednolite niebo,
    \item ograniczenie widzenia do jednej perspektywy, co powoduje występowanie martwych stref pomiędzy kamerami lub poza polem widzenia jednej z nich.
\end{itemize}

Aby stereowizja mogła funkcjonować w sposób użyteczny w systemach antykolizyjnych, konieczne jest zastosowanie dodatkowych czujników, takich jak wodoodporny czujnik ultradźwiękowy A02YYUW czy laserowy czujnik LiDAR. Należy jednak podkreślić, że również te urządzenia posiadają własne ograniczenia.

Kolejnym istotnym problemem jest liczba klatek na sekundę (ang. \textit{Frames Per Second, FPS}). Analiza wyników wykazała, że osiągnięcie wartości około $4\,\text{FPS}$ jest niewystarczające do zapewnienia pewnej i szybkiej reakcji systemu na wykrycie zagrożenia kolizją. Główną przyczyną tak niskiej wydajności jest fakt, iż obliczenia realizowane są na procesorze centralnym (CPU), który wykonuje instrukcje sekwencyjnie.

Dla porównania, przy zastosowaniu wydajniejszego CPU udało się uzyskać około $16\,\text{FPS}$, co wciąż stanowi wartość niewystarczającą.

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.6\textwidth]{images/cpu-compare.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[Porównanie CPU ARM i x86. https://www.cpubenchmark.net/]{Porównanie CPU ARM i x86}
\end{figure}

Rozwiązaniem tego problemu może być zastosowanie odpowiedniego komputera jednopłytkowego z wydajnym procesorem graficznym (GPU), np. z rodziny Jetson firmy NVIDIA.

\begin{figure}[H]  % 'h' means "here", it places the figure at the current location in the document
    \centering  % Centers the image
    \includegraphics[width=0.4\textwidth]{images/jetson.png}  % Adjust the path and width as necessary
    \captionsetup{font=footnotesize}
    \caption[NVIDIA Jetson Nano B01. https://kamami.pl/wycofane-z-oferty/574587-zestaw-deweloperski-nvidia-jetson-nano.html]{NVIDIA Jetson Nano B01.}
\end{figure}

Taka pltaforma umożliwi na przeniesienie omuwionych obliczeń na GPU. Pozwoli to na znaczące zwiększenie liczby przetwarzanych klatek na sekundę, a tym samym poprawi responsywność systemu w kontekście wykrywania kolizji.

Implementacja systemu antykolizyjnego na bazie Raspberry Pi 5 oraz kamery stereoskopowej wykazała, że nawet przy ograniczonych zasobach sprzętowych możliwe jest skuteczne przeprowadzanie przetwarzania obrazu w czasie rzeczywistym. Pomimo istotnego obciążenia obliczeniowego, stereowizja dostarcza bogatych informacji o głębi sceny, co zwiększa niezawodność systemów unikania przeszkód.

Dzięki kompaktowym rozmiarom, niskiej cenie oraz energooszczędności, Raspberry Pi 5 okazuje się atrakcyjną platformą dla zastosowań edukacyjnych, prototypowych oraz potencjalnie komercyjnych.

\renewcommand{\listfigurename}{Spis rysunków}
\addcontentsline{toc}{chapter}{Spis rysunków}
\listoffigures

\begin{thebibliography}{7}
\addcontentsline{toc}{chapter}{Bibliografia}
\raggedright

\bibitem{otworkowa} Fotografia otworkowa. Jak zrobić zdjęcie bez specjalistycznego aparatu fotograficznego?, \textit{https://gaudemater.pl/fotografia-otworkowa/}, 2021-04-23.
\bibitem{ogniskowa} Shawn Ingersoll, Derek Boyd, Kilen Murphy, Khara Plicanic, Anna Goellner, Zapoznanie z pojęciem i zastosowaniami ogniskowej, \textit{https://www.adobe.com/pl/creativecloud/photography/discover/focal-length.html}.
\bibitem{promieniowe} Richard Hartley, Andrew Zisserman, Multiple View Geometry in Computer Vision, \textit{https://www.r-5.org/files/books/computers/algo-list/image-processing/vision/Richard\_Hartley\_Andrew\_Zisserman-Multiple\_View\_Geometry\_in\_Computer\_Vision-EN.pdf}, 2004.
\bibitem{dystorsja} Dave Christian, Brown's Distortion Model and How To Use It \textit{https://www.foamcoreprint.com/blog/what-are-calibration-targets}, 2023-02-20.
\bibitem{lambert} John Lambert, Stereo and Disparity, \textit{https://johnwlambert.github.io/stereo/}.
\bibitem{disparity} Baeldung authors, Disparity Map in Stereo Vision, \textit{https://www.baeldung.com/cs/disparity-map-stereo-vision}, 2025-03-26.
\bibitem{rao} Rajesh Rao, Stereo and 3D Vision, \textit{https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf}.
\bibitem{reilly} Adrian Kaehler, Gary Bradski, Learning OpenCV 3, O'Reilly, 2017-11.
\bibitem{dystorsja} Dystorsja w fotografii, \textit{https://beafoto.pl/dystorsja}, 2021-01.
\bibitem{calib} Kaustubh Sadekar, Making A Low-Cost Stereo Camera Using OpenCV, \textit{https://learnopencv.com/making\_a\_low\_cost\_stereo\_camera\_using\_opencv/}, 2021-01-11.
\bibitem{bm} Sushma Sri, Motion Estimation using Block Matching
Algorithm, \textit{https://media.neliti.com/media/publications/237501-motion-estimation-using-block-matching-a-4e613693.pdf}, 2018-05.
\bibitem{wls} Renzhi Mao, Kaijie Wei, Hideharu Amano, Yuki Kuno, Masatoshi Arai, Weighted Least Square Filter for Improving the Quality of Depth Map on FPGA, \textit{http://www.ijnc.org/index.php/ijnc/article/view/291}, 2022-07.
\bibitem{semi-global} Heiko Hirschmüller \textit{https://en.wikipedia.org/wiki/Semi\_global\_matching}, 2005.
\bibitem{hartley} Ralph Hartley \textit{https://en.wikipedia.org/wiki/Hartley\_function}, 1928.
\bibitem{bouget} Jean-Yves Bouguet \textit{https://robots.stanford.edu/cs223b04/JeanYvesCalib/}.

\end{thebibliography}
\end{document}
